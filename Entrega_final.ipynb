{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f7067a06",
      "metadata": {
        "id": "f7067a06"
      },
      "source": [
        "\n",
        "# **Pre‚ÄëEntrega  ‚Äì Versi√≥n Pedag√≥gica**\n",
        "\n",
        "**Datasets:** `ventas.csv`, `clientes.csv`, `marketing.csv` (en Drive).\n",
        "\n",
        "### Mapeo a la r√∫brica\n",
        "1. Entorno & carga de datos ‚úîÔ∏è  \n",
        "2. No se entrega\n",
        "3. No se entrega\n",
        "4. Exploraci√≥n inicial con Pandas ‚úîÔ∏è  \n",
        "5. Calidad de datos ‚úîÔ∏è  \n",
        "6. Limpieza del dataset ‚úîÔ∏è\n",
        "7. En revisi√≥n\n",
        "8. En revisi√≥n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f515894",
      "metadata": {
        "id": "1f515894"
      },
      "source": [
        "## 1) Entorno y **carga de datos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9789cd42",
      "metadata": {
        "id": "9789cd42"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Montar la unidad\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "Zh8LEkMzkAkp",
        "outputId": "7bc8983f-2305-432b-c8f5-4993a0fe26bf"
      },
      "id": "Zh8LEkMzkAkp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2607721981.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Montar la unidad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar que los archivos csv se encuentren en la carpeta datasets\n",
        "import os\n",
        "os.listdir(\"/content/drive/MyDrive/datasets\")"
      ],
      "metadata": {
        "id": "IOlufknzkDF6"
      },
      "id": "IOlufknzkDF6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos las rutas de los datasets.\n",
        "# En Google Colab, si sub√≠s archivos, pod√©s usar la carpeta /content o montar Drive.\n",
        "# Aqu√≠ dejamos rutas relativas para que sea f√°cil reemplazarlas si cambia la ubicaci√≥n.\n",
        "ruta_ventas = \"/content/drive/MyDrive/datasets/ventas.csv\"\n",
        "ruta_clientes = \"/content/drive/MyDrive/datasets/clientes.csv\"\n",
        "ruta_marketing = \"/content/drive/MyDrive/datasets/marketing.csv\"\n",
        "\n",
        "# Cargamos los CSV como DataFrames.\n",
        "ventas = pd.read_csv(ruta_ventas)\n",
        "clientes = pd.read_csv(ruta_clientes)\n",
        "marketing = pd.read_csv(ruta_marketing)\n",
        "\n",
        "# Validamos formas para comprobar que se cargaron correctamente.\n",
        "print(\"ventas.shape ->\", ventas.shape)\n",
        "print(\"clientes.shape ->\", clientes.shape)\n",
        "print(\"marketing.shape ->\", marketing.shape)\n",
        "\n",
        "# Mostramos las primeras filas de cada dataset para corroborar estructura de columnas.\n",
        "display(ventas.head(3))\n",
        "display(clientes.head(3))\n",
        "display(marketing.head(3))"
      ],
      "metadata": {
        "id": "NsCw0vUCkFR5"
      },
      "id": "NsCw0vUCkFR5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3a7c2204",
      "metadata": {
        "id": "3a7c2204"
      },
      "source": [
        "## 4) **Exploraci√≥n inicial con pandas** (EDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39a9e236",
      "metadata": {
        "id": "39a9e236"
      },
      "outputs": [],
      "source": [
        "\n",
        "def eda(df, nombre):\n",
        "    print(f\"=== {nombre} ===\")\n",
        "    print(\"shape:\", df.shape)\n",
        "    print(\"columnas:\", list(df.columns))\n",
        "    print(\"dtypes:\")\n",
        "    print(df.dtypes)\n",
        "    print(\"\\nNulos por columna:\")\n",
        "    print(df.isna().sum())\n",
        "    print(\"\\nPrimeras filas:\")\n",
        "    display(df.head(5))\n",
        "    print(\"\\nDescribe (num√©rico):\")\n",
        "    display(df.describe(include='number'))\n",
        "    print(\"-\"*100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eda(ventas, \"VENTAS (inicial)\")"
      ],
      "metadata": {
        "id": "Yc6hZcp-k4-C"
      },
      "id": "Yc6hZcp-k4-C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eda(clientes, \"CLIENTES (inicial)\")"
      ],
      "metadata": {
        "id": "S2_hXeh8k0mW"
      },
      "id": "S2_hXeh8k0mW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eda(marketing, \"MARKETING (inicial)\")"
      ],
      "metadata": {
        "id": "v9eBNKbkk1u-"
      },
      "id": "v9eBNKbkk1u-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "39c2a814",
      "metadata": {
        "id": "39c2a814"
      },
      "source": [
        "## 5) **Calidad de datos** (nulos y duplicados)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7585d4bb",
      "metadata": {
        "id": "7585d4bb"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================\n",
        "# üîç FUNCI√ìN DE CONTROL DE CALIDAD DE DATOS\n",
        "# ============================================\n",
        "# Esta funci√≥n analiza un DataFrame existente (df) y muestra:\n",
        "# 1Ô∏è‚É£ La cantidad de valores nulos por columna.\n",
        "# 2Ô∏è‚É£ El total de filas completamente duplicadas.\n",
        "# 3Ô∏è‚É£ Si se especifica una columna clave, los valores m√°s repetidos de esa clave.\n",
        "\n",
        "def calidad(df, nombre, clave=None):\n",
        "    \"\"\"\n",
        "    Analiza la calidad del DataFrame:\n",
        "      - Muestra cantidad de nulos por columna.\n",
        "      - Cuenta filas duplicadas completas.\n",
        "      - Si se indica una clave, muestra los valores duplicados m√°s frecuentes.\n",
        "    Par√°metros:\n",
        "      df: DataFrame de pandas que se analizar√°.\n",
        "      nombre: texto descriptivo del DataFrame (ejemplo: 'VENTAS').\n",
        "      clave: (opcional) nombre de la columna para buscar duplicados espec√≠ficos.\n",
        "    \"\"\"\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # Mostrar t√≠tulo descriptivo con el nombre del DF\n",
        "    # -------------------------------------------------\n",
        "    print(f\"### {nombre}\")\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # Mostrar cantidad de valores nulos por columna\n",
        "    # -------------------------------------------------\n",
        "    # df.isna() devuelve un DataFrame booleano con True donde hay NaN.\n",
        "    # .sum() cuenta los True (o sea, los nulos) por columna.\n",
        "    # .to_frame(\"nulos\") convierte el resultado en un DataFrame con una columna llamada 'nulos'.\n",
        "    display(df.isna().sum().to_frame(\"nulos\"))\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # Contar filas duplicadas completas\n",
        "    # -------------------------------------------------\n",
        "    # df.duplicated(keep=False) marca como True todas las filas que tienen otra igual.\n",
        "    # keep=False significa que marca todas las copias, no solo una.\n",
        "    # .sum() cuenta cu√°ntas filas est√°n repetidas.\n",
        "    dup_rows = df.duplicated(keep=False).sum()\n",
        "    print(\"Filas duplicadas (exactas):\", dup_rows)\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # Si se especific√≥ una columna clave v√°lida, analizar duplicados por esa columna\n",
        "    # -------------------------------------------------\n",
        "    # if clave analiza que clave no sea None\n",
        "    # and (y)\n",
        "    if clave and clave in df.columns:\n",
        "    # clave in df.columns-- >que clave sea una columna existente dentro de las columnas del dataframe\n",
        "    # si no le paso ninguna columna no va a querer encontrar duplicados por columna\n",
        "    # y si me equivoco y le paso una columna que no existe en el dataframe, tampoco ingresara al if.\n",
        "        # Contar cu√°ntas filas tienen valores repetidos en esa columna\n",
        "        dup_key = df[clave].duplicated(keep=False).sum()\n",
        "        print(f\"Duplicados por clave '{clave}':\", dup_key)\n",
        "\n",
        "        # Si existen duplicados, mostrar cu√°les son los valores m√°s repetidos\n",
        "        if dup_key > 0:\n",
        "            # Filtrar filas donde esa clave est√© duplicada\n",
        "            # df[clave].duplicated(keep=False) devuelve True donde el valor se repite\n",
        "            duplicados_ordenados = (\n",
        "                df[df[clave].duplicated(keep=False)][clave]\n",
        "                .value_counts()                # Cuenta cu√°ntas veces aparece cada valor\n",
        "                .sort_values(ascending=False)   # Ordena de mayor a menor (m√°s duplicados arriba)\n",
        "            )\n",
        "\n",
        "            print(\"\\nüîÅ Top valores duplicados m√°s frecuentes:\")\n",
        "            # Mostrar solo los primeros 10 (los m√°s repetidos)\n",
        "            display(duplicados_ordenados.head(10))\n",
        "        else:\n",
        "            print(f\"No se encontraron duplicados en la clave '{clave}'.\")\n",
        "    else:\n",
        "        # Si la clave no fue pasada o no existe en el DataFrame\n",
        "        if clave:\n",
        "            print(f\"La clave '{clave}' no existe en el DataFrame.\")\n",
        "        else:\n",
        "            print(\"No se indic√≥ una clave para analizar duplicados por columna.\")\n",
        "#fin de def calidad\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# ‚úÖ EJEMPLO DE USO (usa tu propio DataFrame existente)\n",
        "# ============================================\n",
        "\n",
        "# Ejemplo: si ya ten√©s un DataFrame llamado 'ventas' con una columna 'id_venta'\n",
        "# ejecut√° as√≠:\n",
        "# calidad(ventas, \"VENTAS\", clave=\"id_venta\")\n",
        "\n",
        "# Si solo quer√©s ver los nulos y duplicados generales (sin clave):\n",
        "# calidad(ventas, \"VENTAS\")\n",
        "\n",
        "\n",
        "# Si tus claves se llaman distinto, ajust√° estos nombres:\n",
        "calidad(ventas, \"VENTAS\", clave=\"id_venta\")\n",
        "\n"
      ],
      "metadata": {
        "id": "C3npMrDXPxhB"
      },
      "id": "C3npMrDXPxhB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calidad(clientes, \"CLIENTES\", clave=\"id_cliente\")"
      ],
      "metadata": {
        "id": "gFFUmXaUlDvz"
      },
      "id": "gFFUmXaUlDvz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calidad(marketing, \"MARKETING\", clave=\"id_campanha\")"
      ],
      "metadata": {
        "id": "6Ah2_dWWlExf"
      },
      "id": "6Ah2_dWWlExf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4aeb4945",
      "metadata": {
        "id": "4aeb4945"
      },
      "source": [
        "\n",
        "## 6) **Limpieza del dataset**\n",
        "- Eliminamos duplicados.\n",
        "- Normalizamos **texto** en columnas `object` (trim + capitalizaci√≥n simple).\n",
        "- Convertimos fechas a fechas reales\n",
        "- Convertimos `precio` y `cantidad` a num√©ricos si existen.\n",
        "- Guardamos CSV limpios.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fd5ccdf",
      "metadata": {
        "id": "9fd5ccdf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================\n",
        "# üßπ LIMPIEZA Y NORMALIZACI√ìN DE LOS DATASETS\n",
        "# ============================================\n",
        "# Se limpian y normalizan los DataFrames:\n",
        "#   ventas, clientes, marketing\n",
        "# ============================================\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 1Ô∏è‚É£ Crear copias independientes para no modificar los originales\n",
        "# -------------------------------------------------\n",
        "ventas_clean = ventas.copy()\n",
        "clientes_clean = clientes.copy()\n",
        "marketing_clean = marketing.copy()\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 2Ô∏è‚É£ Eliminar filas completamente duplicadas\n",
        "# -------------------------------------------------\n",
        "ventas_clean = ventas_clean.drop_duplicates()\n",
        "clientes_clean = clientes_clean.drop_duplicates()\n",
        "marketing_clean = marketing_clean.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calidad(ventas_clean, \"VENTAS CLEAN\", clave=\"id_venta\")"
      ],
      "metadata": {
        "id": "YqcSJk_EOz2m"
      },
      "id": "YqcSJk_EOz2m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------\n",
        "# 3Ô∏è‚É£ Funci√≥n para limpiar texto en columnas tipo string\n",
        "# -------------------------------------------------\n",
        "def normalizar_texto(df):\n",
        "    for col in df.select_dtypes(include=\"object\").columns:\n",
        "        # Se agrupan las operaciones entre par√©ntesis () para escribirlas en varias l√≠neas\n",
        "        # Python eval√∫a todo el bloque como una √∫nica expresi√≥n.\n",
        "        df[col] = (\n",
        "            df[col]\n",
        "            .astype(str)                              # Convierte cualquier tipo a string\n",
        "            # .astype(str)  ‚Üí convierte todo a texto; no tiene par√°metros adicionales.\n",
        "            .str.strip()                               # Elimina espacios al inicio y final\n",
        "            # .str.strip() no necesita argumentos; borra espacios en blanco por defecto.\n",
        "            .str.replace(r\"[\\u200b\\t\\r\\n]\", \"\", regex=True)\n",
        "            # .str.replace(patron, reemplazo, regex=True)\n",
        "            #   patron: expresi√≥n regular que busca caracteres invisibles (\\u200b, tabulaciones, saltos)\n",
        "            #   reemplazo: \"\"  ‚Üí los elimina\n",
        "            #   regex=True indica que 'patron' es una expresi√≥n regular.\n",
        "            .str.replace(\" +\", \" \", regex=True)\n",
        "            # reemplaza \"uno o m√°s espacios consecutivos\" por un solo espacio\n",
        "            .str.title()                               # Convierte a T√≠tulo: \"juan p√©rez\" ‚Üí \"Juan P√©rez\"\n",
        "        )\n",
        "        #df_transformado=df[col].astype(str)\n",
        "        #df_transformado=df_transformado.str.strip()\n",
        "        #df_transformado=df_transformado.str.replace(r\"[\\u200b\\t\\r\\n]\", \"\", regex=True)\n",
        "        #df_transformado=df_transformado.str.replace(\" +\", \" \", regex=True)\n",
        "        #df_transformado=df_transformado.str.title()\n",
        "        #df[col]=df_transformado\n",
        "\n",
        "        #df[col] = df[col].astype(str).str.strip().str.replace(r\"[\\u200b\\t\\r\\n]\", \"\", regex=True).str.replace(\" +\", \" \", regex=True).str.title()\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "5uLvTPGdlW0L"
      },
      "id": "5uLvTPGdlW0L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------\n",
        "# Normalizar fechas\n",
        "# -------------------------------------------------\n",
        "# Si alguna columna contiene fechas (por ejemplo \"fecha\" o \"fechanotificacion\"),\n",
        "# se intenta convertir a formato datetime de pandas.\n",
        "# to_datetime intenta interpretar el formato y transforma valores inv√°lidos en NaT (Not a Time).\n",
        "\n",
        "for df in [ventas_clean, clientes_clean, marketing_clean]:\n",
        "    for col in df.columns:\n",
        "        if \"fecha\" in col.lower():  # detecta columnas con la palabra \"fecha\"\n",
        "            df[col] = pd.to_datetime(df[col], errors=\"coerce\", dayfirst=True)\n",
        "            # Par√°metros:\n",
        "            #   errors=\"coerce\" ‚Üí convierte valores no v√°lidos en NaT (evita error)\n",
        "            #   dayfirst=True   ‚Üí interpreta formatos tipo \"DD/MM/YYYY\" (formato latino)\n",
        "#n\n"
      ],
      "metadata": {
        "id": "H1kQjCY6lajI"
      },
      "id": "H1kQjCY6lajI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#NORMALIZO FECHAS DE DF DE VENTAS(ESTO TAMBIEN ESTA PERFECTO!!, es lo mismo de arriba, pero sabiendo los nombres de las fechas)\n",
        "\n",
        "ventas_clean[\"fecha_venta\"] = pd.to_datetime(ventas_clean[\"fecha_venta\"], errors=\"coerce\", dayfirst=True)"
      ],
      "metadata": {
        "id": "izf-S1reRJnt"
      },
      "id": "izf-S1reRJnt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#NORMALIZO FECHAS DE DF DE MARKETING\n",
        "\n",
        "marketing_clean[\"fecha_inicio\"] = pd.to_datetime(marketing_clean[\"fecha_inicio\"], errors=\"coerce\", dayfirst=True)\n",
        "marketing_clean[\"fecha_fin\"] = pd.to_datetime(marketing_clean[\"fecha_fin\"], errors=\"coerce\", dayfirst=True)"
      ],
      "metadata": {
        "id": "-Q8SuGBRRVnI"
      },
      "id": "-Q8SuGBRRVnI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ventas_clean.dtypes)\n",
        "print(clientes_clean.dtypes)\n",
        "print(marketing_clean.dtypes)"
      ],
      "metadata": {
        "id": "usC5MsdpQW6V"
      },
      "id": "usC5MsdpQW6V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------\n",
        "#  Aplicar la normalizaci√≥n de texto\n",
        "# -------------------------------------------------\n",
        "ventas_clean = normalizar_texto(ventas_clean)\n",
        "clientes_clean = normalizar_texto(clientes_clean)\n",
        "marketing_clean = normalizar_texto(marketing_clean)"
      ],
      "metadata": {
        "id": "IBlLVsv2lpy-"
      },
      "id": "IBlLVsv2lpy-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mostramos los df luego de normalizar los textos para revisar que queden bien\n",
        "print(ventas_clean.head(10))\n",
        "print(clientes_clean.head(10))\n",
        "print(marketing_clean.head(10))"
      ],
      "metadata": {
        "id": "YPGXQQtBShKr"
      },
      "id": "YPGXQQtBShKr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------\n",
        "# 6Ô∏è‚É£ Normalizar valores num√©ricos\n",
        "# -------------------------------------------------\n",
        "# üè∑Ô∏è Campo \"precio\"\n",
        "if \"precio\" in ventas_clean.columns:\n",
        "    # Se usa nuevamente agrupaci√≥n con () para encadenar m√©todos y mantener legibilidad\n",
        "    ventas_clean[\"precio\"] = (\n",
        "        ventas_clean[\"precio\"]\n",
        "        .astype(str)                        # Convierte todo a texto\n",
        "        .str.replace(\"$\", \"\", regex=False)  # Elimina el s√≠mbolo $\n",
        "        #   \"$\" ‚Üí texto literal a reemplazar\n",
        "        #   \"\"  ‚Üí nuevo valor (vac√≠o)\n",
        "        #   regex=False ‚Üí interpreta \"$\" literalmente, no como expresi√≥n regular\n",
        "        .str.replace(\",\", \"\", regex=False)  # Elimina comas de miles 1,000  1000\n",
        "        .str.strip()                        # Quita espacios sobrantes\n",
        "    )\n",
        "    ventas_clean[\"precio\"] = pd.to_numeric(ventas_clean[\"precio\"], errors=\"coerce\")\n",
        "    # pd.to_numeric convierte texto a n√∫mero (float o int)\n",
        "    # Par√°metros:\n",
        "    #   errors=\"coerce\" ‚Üí reemplaza valores no convertibles con NaN\n"
      ],
      "metadata": {
        "id": "OE6jSeXSlumM"
      },
      "id": "OE6jSeXSlumM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ventas_clean.dtypes)"
      ],
      "metadata": {
        "id": "L-DTI5UmT8US"
      },
      "id": "L-DTI5UmT8US",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ventas_clean.columns)"
      ],
      "metadata": {
        "id": "a8HrV6GQTy0r"
      },
      "id": "a8HrV6GQTy0r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üßÆ Campo \"cantidad\"\n",
        "if \"cantidad\" in ventas_clean.columns:\n",
        "    ventas_clean[\"cantidad\"] = pd.to_numeric(\n",
        "        ventas_clean[\"cantidad\"], errors=\"coerce\"\n",
        "    ).astype(\"Int64\")\n",
        "    # .astype(\"Int64\") usa el tipo entero de pandas que permite valores nulos (NaN)"
      ],
      "metadata": {
        "id": "GUyZNGi4lyy5"
      },
      "id": "GUyZNGi4lyy5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ventas_clean.dtypes)"
      ],
      "metadata": {
        "id": "7JgBIoBFUPei"
      },
      "id": "7JgBIoBFUPei",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------\n",
        "# 7Ô∏è‚É£ Guardar los DataFrames limpios como CSV\n",
        "# -------------------------------------------------\n",
        "#print(ventas_clean.head())\n",
        "#print(clientes_clean.head())\n",
        "#print(marketing_clean.head())\n",
        "ventas_clean.info()\n",
        "ventas_clean.to_csv(\"/content/drive/MyDrive/datasets/ventas_clean.csv\", index=False)\n",
        "clientes_clean.to_csv(\"/content/drive/MyDrive/datasets/clientes_clean.csv\", index=False)\n",
        "marketing_clean.to_csv(\"/content/drive/MyDrive/datasets/marketing_clean.csv\", index=False)\n",
        "\n",
        "print(\"‚úÖ Archivos guardados: ventas_clean.csv, clientes_clean.csv, marketing_clean.csv\")"
      ],
      "metadata": {
        "id": "vYF4tjGkl3bF"
      },
      "id": "vYF4tjGkl3bF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ventas_clean.select_dtypes(include=\"object\").columns)"
      ],
      "metadata": {
        "id": "nCDTQZMCZ7x3"
      },
      "id": "nCDTQZMCZ7x3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reporte Global luego de la limpieza de datos"
      ],
      "metadata": {
        "id": "dcteETkI_GZd"
      },
      "id": "dcteETkI_GZd"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# üìä REPORTE GLOBAL DE CALIDAD DE DATOS\n",
        "# ============================================\n",
        "# Esta funci√≥n lee los tres DataFrames limpios (o los recibe en memoria)\n",
        "# y muestra un resumen comparativo de nulos, duplicados y tipos de datos.\n",
        "# ============================================\n",
        "\n",
        "def reporte_calidad_global(dfs, nombres):\n",
        "    \"\"\"\n",
        "    Crea un resumen de calidad de varios DataFrames.\n",
        "\n",
        "    Par√°metros:\n",
        "      dfs: lista de DataFrames (por ejemplo [ventas_clean, clientes_clean, marketing_clean])\n",
        "      nombres: lista de nombres correspondientes ([\"VENTAS\", \"CLIENTES\", \"MARKETING\"])\n",
        "    \"\"\"\n",
        "    resumen = []\n",
        "    #zip-->es una funci√≥n incorporada de Python que une elementos de dos (o m√°s) iterables\n",
        "    # ‚Äîpor ejemplo, listas, tuplas o cualquier objeto iterable‚Äî en pares ordenados.\n",
        "    for df, nombre in zip(dfs, nombres):\n",
        "        nulos = df.isna().sum().sum()                    # Total de valores nulos, no por columnas sino total, por eso el doble sum\n",
        "        duplicados = df.duplicated(keep=False).sum()     # Total de filas duplicadas\n",
        "        columnas = len(df.columns)                       # Cantidad de columnas\n",
        "        filas = len(df)                                  # Cantidad de registros\n",
        "\n",
        "        resumen.append({\n",
        "            \"Dataset\": nombre,\n",
        "            \"Filas\": filas,\n",
        "            \"Columnas\": columnas,\n",
        "            \"Nulos totales\": nulos,\n",
        "            \"Duplicados\": duplicados,\n",
        "        })\n",
        "\n",
        "    reporte = pd.DataFrame(resumen)\n",
        "    #display(reporte)\n",
        "    return reporte\n",
        "\n",
        "# ============================================\n",
        "# ‚úÖ Ejemplo de uso\n",
        "# ============================================\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NMXGZ9Ri_LKa"
      },
      "id": "NMXGZ9Ri_LKa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(reporte_calidad_global([ventas, clientes, marketing], [\"VENTAS Original\", \"CLIENTES Original\", \"MARKETING Original\"]))\n",
        "print(reporte_calidad_global([ventas_clean, clientes_clean, marketing_clean],[\"VENTAS Copia   \", \"CLIENTES Copia   \", \"MARKETING Copia   \"]))"
      ],
      "metadata": {
        "id": "gZyVQzkVYJmp"
      },
      "id": "gZyVQzkVYJmp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##üü¶ Punto 7 ‚Äî Transformaci√≥n de datos (filtrar ‚Äúalto rendimiento‚Äù)\n",
        "\n",
        "\n",
        "### Objetivo: construir una tabla de rendimiento por producto y quedarnos s√≥lo con los productos de alto rendimiento.\n",
        "\n",
        "Conceptos clave:\n",
        "\n",
        "<h3>Transformaci√≥n de datos:</h3> son operaciones que crean/derivan nuevas columnas (por ejemplo ingreso = precio * cantidad), normalizan formatos (texto/fechas/n√∫meros) o filtran filas seg√∫n un criterio.\n",
        "\n",
        "<h3>M√©trica de ingreso:</h3> para ventas, una m√©trica t√≠pica es ingreso por registro = precio * cantidad. Luego podemos agregar por producto (sumar ingresos y unidades) para medir rendimiento total por producto.\n",
        "\n",
        "<h3>Agregaci√≥n:</h3> es resumir muchas filas en pocas, aplicando funciones como sum(), mean(), count() agrupando por una clave (ej., producto).\n",
        "Ej.: ‚Äúingreso total por producto‚Äù = suma de todos los ingresos de ese producto.\n",
        "\n",
        "<h3>Percentil:</h3> el percentil 80 (P80) es un valor tal que el 80% de los datos est√°n por debajo o igual a ese valor y el 20% restante por encima.\n",
        "\n",
        "Si ingreso_total P80 = 120.000, significa que el 80% de los productos tienen ingreso_total ‚â§ 120.000 y el 20% ‚â• 120.000.\n",
        "\n",
        "<h3>Alto rendimiento:</h3> aqu√≠ lo definimos como top 20% de productos seg√∫n ingreso_total (>= P80). Es un criterio com√∫n cuando no hay umbrales de negocio expl√≠citos.\n",
        "Alternativas v√°lidas: top-K (p. ej. top 50 productos), percentil 75 (P75) o un umbral fijo de negocio (p. ej., ‚Äú>= $100.000/mes‚Äù), o score estandarizado (z-score).\n",
        "\n",
        "<h3>Plan paso a paso:</h3>\n",
        "\n",
        "Detectar la columna de producto (tolerando distintos nombres: producto, id_producto, sku, articulo‚Ä¶).\n",
        "\n",
        "Calcular ingreso por registro = precio * cantidad.\n",
        "\n",
        "Agregar por producto para obtener m√©tricas (ingreso_total, unidades, precio_promedio, registros).\n",
        "\n",
        "Calcular P80 con quantile(q=0.80).\n",
        "\n",
        "Filtrar productos con ingreso_total >= P80.\n",
        "\n",
        "Ordenar de mayor a menor."
      ],
      "metadata": {
        "id": "AVsfb1S5CMje"
      },
      "id": "AVsfb1S5CMje"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 7) TRANSFORMACI√ìN: productos de alto rendimiento\n",
        "# ============================================\n",
        "# Objetivo:\n",
        "# - Detectar los productos con mejor desempe√±o econ√≥mico (top 20% por ingreso total).\n",
        "# - Aplicar transformaci√≥n: calcular ingreso, agregar por producto y filtrar.\n",
        "# ============================================\n",
        "\n",
        "def encontrar_columna(df, candidatos):\n",
        "    \"\"\"\n",
        "    Busca la primera columna cuyo nombre contenga alguno de los patrones dados.\n",
        "    - df: DataFrame de pandas.\n",
        "    - candidatos: lista de patrones (min√∫sculas).\n",
        "    \"\"\"\n",
        "    # Recorremos todas las columnas del DataFrame\n",
        "    for c in df.columns:\n",
        "\n",
        "        # Convertimos el nombre de la columna a min√∫sculas\n",
        "        # Esto se hace para comparar sin importar si est√° escrito con may√∫sculas o min√∫sculas\n",
        "        nombre = c.lower()\n",
        "\n",
        "        # Verificamos si alguna palabra (patr√≥n) de la lista 'candidatos'\n",
        "        # est√° contenida dentro del nombre de la columna\n",
        "        # 'any()' devuelve True apenas encuentra una coincidencia\n",
        "        if any(p in nombre for p in candidatos):\n",
        "            # Si encontramos una coincidencia, devolvemos el nombre original de la columna\n",
        "            return c\n",
        "\n",
        "    # Si termina el bucle y no se encontr√≥ ninguna coincidencia, devolvemos None\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "qGMl5rBqCyRW"
      },
      "id": "qGMl5rBqCyRW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 1Ô∏è‚É£ Detectar la columna de producto\n",
        "prod_col = encontrar_columna(ventas_clean, [\"producto\", \"id_producto\", \"sku\", \"articulo\", \"art√≠culo\"])\n",
        "if prod_col is None:\n",
        "    raise ValueError(\"No se encontr√≥ columna de producto. Renombr√° una columna a 'producto' o similar.\")\n",
        "print(prod_col)\n",
        "\n"
      ],
      "metadata": {
        "id": "c-tjuy91b6gH"
      },
      "id": "c-tjuy91b6gH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2Ô∏è‚É£ Calcular ingreso por registro = precio * cantidad\n",
        "#() es para escribir en varias filas\n",
        "ventas_perf = (\n",
        "    ventas_clean\n",
        "    .assign(\n",
        "        ingreso = ventas_clean[\"precio\"] * ventas_clean[\"cantidad\"]\n",
        "        # assign(**nuevas_col): crea nuevas columnas y devuelve una copia del DF.\n",
        "        # Alternativa: ventas_clean[\"ingreso\"] = ventas_clean[\"precio\"] * ventas_clean[\"cantidad\"]\n",
        "    )\n",
        ")\n",
        "#esta linea comentada es igual que la linea multiple de arriba\n",
        "#ventas_perf = ventas_clean.assign(ingreso = ventas_clean[\"precio\"] * ventas_clean[\"cantidad\"])\n",
        "#esta otra linea agrega a ventas_clean una columna nueva ingreso y le asigna precio*cantidad\n",
        "#ventas_clean[\"ingreso\"] = ventas_clean[\"precio\"] * ventas_clean[\"cantidad\"]\n"
      ],
      "metadata": {
        "id": "JcM0dleOcC7T"
      },
      "id": "JcM0dleOcC7T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 3Ô∏è‚É£ Agregar m√©tricas por producto\n",
        "resumen_prod = (\n",
        "    ventas_perf\n",
        "    # 1) Agrupamos el DataFrame por una o varias columnas clave\n",
        "    .groupby(\n",
        "        by=prod_col,    # Columna (str) o lista de columnas (list[str]) que define los grupos.\n",
        "        dropna=False,   # False ‚Üí NO descarta filas donde la clave de grupo tenga NaN; crea un grupo para NaN.\n",
        "        as_index=False, # False ‚Üí las columnas de agrupaci√≥n quedan como columnas normales (no pasan al √≠ndice).\n",
        "        observed=False  # Solo aplica si 'prod_col' es Categorical:\n",
        "                        #   False ‚Üí incluye categor√≠as NO observadas (posibles pero sin filas);\n",
        "                        #   True  ‚Üí solo categor√≠as que aparecen en los datos (m√°s r√°pido y ‚Äúcompacto‚Äù).\n",
        "    )\n",
        "    # 2) Agregamos (resumimos) columnas num√©ricas por cada grupo\n",
        "    .agg(\n",
        "        ingreso_total=('ingreso', 'sum'),   # Suma de 'ingreso' por grupo (skipna=True por defecto).\n",
        "        unidades=('cantidad', 'sum'),       # Suma de 'cantidad' por grupo.\n",
        "        precio_promedio=('precio', 'mean'), # Promedio simple de 'precio' por grupo (ignora NaN).\n",
        "        registros=('ingreso', 'size')       # N√∫mero de filas en el grupo (cuenta TODO, incluso NaN).\n",
        "    )\n",
        ")\n",
        "#se puede escribir asi:\n",
        "resumen_prod = ventas_perf.groupby(by=prod_col).agg(ingreso_total=('ingreso', 'sum'), unidades=('cantidad', 'sum'), precio_promedio=('precio', 'mean'), registros=('ingreso', 'size'))\n",
        "\n"
      ],
      "metadata": {
        "id": "Jt021TpZcLG8"
      },
      "id": "Jt021TpZcLG8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(resumen_prod.head(10))\n",
        "#ordenar resumen_prod por el mayor ingreso_total, y redondear precio_promedio a 2 decimales redondeado"
      ],
      "metadata": {
        "id": "qZ1tg9Z0fuLg"
      },
      "id": "qZ1tg9Z0fuLg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 4Ô∏è‚É£ Calcular percentil 80 de ingreso_total\n",
        "# --------------------------------------------------------\n",
        "# La funci√≥n quantile() nos permite obtener el valor de un percentil.\n",
        "# En este caso, queremos saber el ingreso que separa al 80% de los productos\n",
        "# con menores ingresos del 20% con mayores ingresos.\n",
        "\n",
        "p80_ingreso = resumen_prod[\"ingreso_total\"].quantile(\n",
        "    q=0.80,                # q indica el percentil deseado (0.80 = 80% de los datos por debajo)\n",
        "    interpolation=\"linear\" # si el percentil no coincide exactamente con un valor real del dataset,\n",
        "                           # 'linear' interpola entre los dos valores vecinos.\n",
        "                           # Ejemplo: si el 80% cae entre 4000 y 5000,\n",
        "                           # calcula un valor proporcional, por ejemplo 4200.\n",
        "                           # Otros m√©todos posibles:\n",
        "                           #  - 'lower': toma el menor de los dos valores (4000)\n",
        "                           #  - 'higher': toma el mayor (5000)\n",
        "                           #  - 'nearest': el m√°s cercano al percentil\n",
        "                           #  - 'midpoint': el punto medio exacto (4500)\n",
        ")\n",
        "\n",
        "# En resumen:\n",
        "# - quantile calcula el valor l√≠mite de un percentil.\n",
        "# - q define qu√© percentil queremos.\n",
        "# - interpolation define c√≥mo se calcula cuando el valor no est√° exactamente en los datos.\n",
        "# El resultado (p80_ingreso) es el ingreso total que marca el l√≠mite superior del 80% de los productos.\n",
        "\n",
        "\n",
        "# 5Ô∏è‚É£ Filtrar los productos \"de alto rendimiento\" y ordenarlos\n",
        "# -------------------------------------------------------------------\n",
        "# Contexto: `resumen_prod` es un DataFrame con m√©tricas por producto,\n",
        "# y `p80_ingreso` es el percentil 80 de la columna \"ingreso_total\".\n",
        "# Objetivo: quedarnos con los productos cuyo ingreso_total est√° en el 20% superior\n",
        "# (ingreso_total >= p80_ingreso) y luego ordenarlos de mayor a menor por ingreso y unidades.\n",
        "# en una sola fila\n",
        "#ventas_top = resumen_prod.query(\"ingreso_total >= @p80_ingreso\",engine=\"python\").sort_values(by=[\"ingreso_total\", \"unidades\"], ascending=[False, False], na_position=\"last\", ignore_index=True\n",
        "ventas_top = (\n",
        "    resumen_prod\n",
        "    # ---------------------------------------------------------------\n",
        "    # .query(expr, *, inplace=False, engine='python'|'numexpr')\n",
        "    #   - Aplica un filtro usando una expresi√≥n estilo SQL-simple.\n",
        "    #   - `expr` es un string que se eval√∫a sobre los nombres de las columnas.\n",
        "    #   - Para usar variables de Python (no columnas), se antepone '@' (ej.: @p80_ingreso).\n",
        "    #   - NaN en comparaciones (>, >=, ==, etc.) se comportan como False ‚Üí esas filas no pasan el filtro.\n",
        "    #   - engine='python': interpreta la expresi√≥n con Python puro (compatible siempre).\n",
        "    #   - engine='numexpr': si est√° instalado, acelera operaciones num√©ricas vectorizadas.\n",
        "    #   - inplace: False (por defecto) devuelve un DF nuevo; True modifica el DF original (menos recomendado en cadenas).\n",
        "    .query(\n",
        "        \"ingreso_total >= @p80_ingreso\",  # expr: filtra filas donde ingreso_total es al menos el umbral del P80\n",
        "        engine=\"python\"                   # motor de evaluaci√≥n (usar 'numexpr' si lo ten√©s y quer√©s performance)\n",
        "        # Notas de sintaxis de `expr`:\n",
        "        #   ‚Ä¢ Operadores l√≥gicos: and / or / not   (tambi√©n valen &, |, ~ con par√©ntesis).\n",
        "        #   ‚Ä¢ Strings deben ir entre comillas: canal == 'Online'\n",
        "        #   ‚Ä¢ Columnas con espacios o caracteres raros: usar `backticks`, ej.: `nombre producto` == 'X'\n",
        "        #   ‚Ä¢ Ejemplos:\n",
        "        #       \"ingreso_total >= @p80_ingreso and unidades >= 10\"\n",
        "        #       \"`nombre producto`.str.contains('Promo')\"\n",
        "        #       \"precio_promedio.between(1000, 3000, inclusive='both')\"\n",
        "    )\n",
        "    # ---------------------------------------------------------------\n",
        "    # .sort_values(by, axis=0, ascending=True|[...], inplace=False,\n",
        "    #              kind='quicksort'|'mergesort'|'heapsort'|'stable',\n",
        "    #              na_position='last'|'first', ignore_index=False, key=None)\n",
        "    #   - Ordena por una o varias columnas.\n",
        "    #   - `by`: str o lista de str con las columnas a ordenar.\n",
        "    #   - `ascending`: bool o lista de bool (una por cada columna en `by`).\n",
        "    #   - `na_position`: d√≥nde ubicar NaN ('last' o 'first').\n",
        "    #   - `ignore_index`: si True, reasigna el √≠ndice 0..n-1 en el resultado.\n",
        "    #   - `kind`: algoritmo de ordenamiento (mergesort es estable si necesit√°s preservar empates).\n",
        "    #   - `key`: funci√≥n que transforma los valores antes de ordenar (p. ej., key=lambda s: s.str.normalize(...)).\n",
        "    .sort_values(\n",
        "        by=[\"ingreso_total\", \"unidades\"],  # primero ordena por ingreso_total, luego desempata por unidades\n",
        "        ascending=[False, False],          # ambos en orden descendente (mayor ‚Üí menor)\n",
        "        na_position=\"last\",                # coloca NaN al final (√∫til si alguna m√©trica qued√≥ en NaN)\n",
        "        ignore_index=True                  # reindexa el resultado secuencialmente (0..n-1)\n",
        "        # Variantes √∫tiles:\n",
        "        #   ‚Ä¢ ascending=True                 # orden ascendente\n",
        "        #   ‚Ä¢ ascending=[False, True]        # primero desc, luego asc para el segundo criterio\n",
        "        #   ‚Ä¢ kind='mergesort'               # orden estable (respeta el orden de aparici√≥n en empates)\n",
        "        #   ‚Ä¢ key=lambda s: s.str.lower()    # ordenar texto sin distinci√≥n de may√∫sculas/min√∫sculas\n",
        "    )\n",
        ")\n",
        "\n",
        "# Resultado:\n",
        "# `ventas_top` contiene solo los productos cuyo ingreso_total >= p80_ingreso,\n",
        "# ordenados de mayor a menor por ingreso_total y, ante empates, por unidades.\n",
        "\n",
        "\n",
        "# 6Ô∏è‚É£ Mostrar resultados\n",
        "print(f\"Columna de producto detectada: {prod_col}\")\n",
        "print(f\"Umbral (percentil 80) de ingreso_total: {float(p80_ingreso):,.2f}\")\n",
        "print(\"‚úÖ Productos de ALTO RENDIMIENTO (top 20% por ingreso):\")\n",
        "display(ventas_top.head(20))"
      ],
      "metadata": {
        "id": "CTXetAl8czea"
      },
      "id": "CTXetAl8czea",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ESTO ES SOLO UN EJEMPLO PARA ENTENDER PERCENTIL Y NO TIENE NADA QUE VER CON LA ENTREGA!!!\n",
        "\n",
        "# Lista de ingresos ordenados\n",
        "ingresos = pd.Series([100, 200, 300, 400, 500])\n",
        "\n",
        "# Calculamos el percentil 80 (P80)\n",
        "# ---------------------------------------------------------\n",
        "# Interpolar significa estimar un valor intermedio entre dos puntos conocidos.\n",
        "# El percentil 80 no siempre coincide con un valor exacto del dataset,\n",
        "# sino que puede \"caer entre\" dos posiciones.\n",
        "#\n",
        "# F√≥rmula de posici√≥n usada por pandas:\n",
        "#   posici√≥n = (n - 1) * q\n",
        "# En este caso: (5 - 1) * 0.8 = 3.2 ‚Üí est√° entre los √≠ndices 3 (valor 400) y 4 (valor 500)\n",
        "#\n",
        "# Interpolaci√≥n lineal:\n",
        "#   400 + 0.2 * (500 - 400) = 420\n",
        "\n",
        "p80_linear   = ingresos.quantile(0.8, interpolation='linear')\n",
        "p80_lower    = ingresos.quantile(0.8, interpolation='lower')\n",
        "p80_higher   = ingresos.quantile(0.8, interpolation='higher')\n",
        "p80_nearest  = ingresos.quantile(0.8, interpolation='nearest')\n",
        "p80_midpoint = ingresos.quantile(0.8, interpolation='midpoint')\n",
        "\n",
        "print(\"Lista de ingresos:\", list(ingresos))\n",
        "print(\"\\nPercentil 80 con diferentes m√©todos de interpolaci√≥n:\\n\")\n",
        "print(f\"Linear   ‚Üí {p80_linear}   (interpola ‚Üí 420)\")\n",
        "print(f\"Lower    ‚Üí {p80_lower}    (toma el menor ‚Üí 400)\")\n",
        "print(f\"Higher   ‚Üí {p80_higher}   (toma el mayor ‚Üí 500)\")\n",
        "print(f\"Nearest  ‚Üí {p80_nearest}  (toma el m√°s cercano ‚Üí 400)\")\n",
        "print(f\"Midpoint ‚Üí {p80_midpoint} (punto medio ‚Üí 450)\")\n",
        "\n",
        "# En resumen:\n",
        "# - 'linear' estima el valor intermedio (420).\n",
        "# - 'lower', 'higher', 'nearest', 'midpoint' eligen seg√∫n criterio fijo.\n",
        "# - Interpolar = estimar un valor entre los existentes seg√∫n la posici√≥n que ocupa el percentil.\n",
        "\n"
      ],
      "metadata": {
        "id": "WFGIJRMvruzk"
      },
      "id": "WFGIJRMvruzk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üü¶ Punto 8 ‚Äî Agregaci√≥n (resumen por categor√≠a y an√°lisis de ingresos)\n",
        "### Conceptos y plan\n",
        "\n",
        "Objetivo: construir un resumen por categor√≠a de producto con m√©tricas √∫tiles (ingreso total, unidades, cantidad de ventas, ticket promedio).\n",
        "\n",
        "Conceptos clave:\n",
        "\n",
        "Agregaci√≥n: operaci√≥n que resume muchas filas en menos filas, aplicando funciones (sum, mean, count, etc.) despu√©s de agrupar por una clave (aqu√≠, la categor√≠a).\n",
        "\n",
        "Categor√≠a de producto: atributo que agrupa productos similares (ej., ‚ÄúElectr√≥nica‚Äù, ‚ÄúHogar‚Äù). Puede llamarse categoria, rubro, etc.\n",
        "\n",
        "Ticket promedio por venta: ingreso_total / ventas (d√≥nde ventas es el conteo de filas en esa categor√≠a). Indica el importe medio facturado por cada transacci√≥n/registro en la categor√≠a.\n",
        "\n",
        "Nota: esto no es el ‚Äúprecio promedio‚Äù del producto; ese ya se calcula con mean sobre precio.\n",
        "\n",
        "Consideraciones: outliers pueden distorsionar promedios; a veces conviene mirar tambi√©n la mediana (median).\n",
        "\n",
        "Plan paso a paso:\n",
        "\n",
        "Detectar la columna de categor√≠a.\n",
        "\n",
        "Asegurar columna ingreso (si no existe, crearla).\n",
        "\n",
        "groupby(categor√≠a).agg(...) para obtener m√©tricas.\n",
        "\n",
        "Ordenar por ingreso_total.\n",
        "\n",
        "Calcular ticket_promedio_por_venta."
      ],
      "metadata": {
        "id": "EZQ3mv-rDoGz"
      },
      "id": "EZQ3mv-rDoGz"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 8) AGREGACI√ìN: resumen por categor√≠a\n",
        "# ============================================\n",
        "# Requisitos:\n",
        "# - 'ventas_clean' tiene columnas 'precio', 'cantidad' y alguna columna \"categor√≠a\".\n",
        "# ============================================\n",
        "\n",
        "# -- Helper para detectar la columna de categor√≠a --\n",
        "def encontrar_columna(df, candidatos):\n",
        "    \"\"\"\n",
        "    df: DataFrame de pandas que queremos inspeccionar.\n",
        "    candidatos: iterable de strings con patrones posibles para el nombre de la columna\n",
        "                (por ejemplo: [\"categoria\", \"cat\", \"segmento\"]).\n",
        "    Devuelve:\n",
        "      - El nombre de la PRIMERA columna cuyo nombre contenga alguno de los patrones (b√∫squeda por subcadena,\n",
        "        sin distinguir may√∫sculas/min√∫sculas).\n",
        "      - None si no encuentra coincidencias.\n",
        "    \"\"\"\n",
        "\n",
        "    # Recorremos todos los nombres de columnas del DataFrame (df.columns es un Index con esos nombres).\n",
        "    for c in df.columns:\n",
        "\n",
        "        # any(...) devuelve True si AL MENOS UNO de los elementos del generador interno es True.\n",
        "        # Generador: (p in c.lower() for p in candidatos)\n",
        "        #   - c.lower(): pasamos el nombre de la columna a min√∫sculas para comparar sin importar may√∫sculas/min√∫sculas.\n",
        "        #   - p in c.lower(): chequea si el patr√≥n 'p' aparece como SUBCADENA dentro del nombre de la columna.\n",
        "        #   - Se eval√∫a para cada 'p' en 'candidatos'.\n",
        "        if any(p in c.lower() for p in candidatos):\n",
        "           # Si encontramos una coincidencia, devolvemos inmediatamente el nombre ORIGINAL de la columna.\n",
        "            return c\n",
        "    # Si terminamos el bucle sin encontrar coincidencias, devolvemos None para indicar \"no encontrada\".\n",
        "    return None\n",
        "\n",
        "# Nota did√°ctica:\n",
        "# - Esto hace coincidencia por SUBCADENA (ej.: \"prod\" matchea \"producto_sku\").\n",
        "# - Si quer√©s coincidencia EXACTA, us√° igualdad (c.lower() == p) o expresiones regulares.\n",
        "# - Si tus datos pueden tener acentos/espacios raros, pod√©s normalizar:\n",
        "#     import unicodedata, re\n",
        "#     def norm(s): return re.sub(r'\\s+', ' ', ''.join(ch for ch in unicodedata.normalize('NFKD', s)\n",
        "#                                    if not unicodedata.combining(ch))).strip().lower()\n",
        "\n",
        "\n",
        "# 1) Detectar columna de categor√≠a (acepta variantes)\n",
        "cat_col = encontrar_columna(ventas_clean, [\"categoria\", \"categor√≠a\", \"categoria_producto\", \"rubro\"])\n",
        "if cat_col is None:\n",
        "    raise ValueError(\"No se encontr√≥ columna de categor√≠a (por ej. 'categoria' o 'rubro').\")\n",
        "\n",
        "# 2) Asegurar columna 'ingreso' (si no existe, crearla)\n",
        "if \"ingreso\" not in ventas_clean.columns:\n",
        "    ventas_cat = ventas_clean.assign(ingreso = ventas_clean[\"precio\"] * ventas_clean[\"cantidad\"])\n",
        "else:\n",
        "    ventas_cat = ventas_clean.copy()\n",
        "\n",
        "# 3) Agregaci√≥n por categor√≠a con groupby + agg\n",
        "resumen_cat = (\n",
        "    ventas_cat\n",
        "    .groupby(\n",
        "        by=cat_col,      # Puede ser string o lista de strings si quisi√©ramos agrupar por varias columnas.\n",
        "        dropna=False,    # Mantener grupo NaN (si hay filas sin categor√≠a).\n",
        "        as_index=False   # Dejar la categor√≠a como columna normal (y no como √≠ndice).\n",
        "        # observed: si cat_col es 'category' y queremos mostrar solo categor√≠as presentes ‚Üí True.\n",
        "    )\n",
        "    .agg(\n",
        "        ingreso_total=('ingreso', 'sum'),   # Suma total de ingresos por categor√≠a.\n",
        "        unidades=('cantidad', 'sum'),       # Unidades totales vendidas en la categor√≠a.\n",
        "        ventas=('ingreso', 'size'),         # Cantidad de registros/filas (ventas) en la categor√≠a.\n",
        "        precio_promedio=('precio', 'mean')  # Precio promedio observado en la categor√≠a.\n",
        "        # Otras funciones √∫tiles: 'median','max','min','std','var','nunique'...\n",
        "    )\n",
        "    .sort_values(\n",
        "        by='ingreso_total', # Ordenar por ingreso total\n",
        "        ascending=False,    # Descendente: mayores arriba\n",
        "        na_position='last', # NaN al final\n",
        "        ignore_index=True   # Reindexar desde 0\n",
        "    )\n",
        ")\n",
        "\n",
        "# 4) Ticket promedio por venta = ingreso_total / ventas\n",
        "resumen_cat = resumen_cat.assign(\n",
        "    ticket_promedio_por_venta = resumen_cat['ingreso_total'] / resumen_cat['ventas']\n",
        "    # assign: crea/reescribe columnas. Alternativa: resumen_cat['ticket_promedio_por_venta'] = ...\n",
        ")\n",
        "\n",
        "print(\"Columna de categor√≠a detectada:\", cat_col)\n",
        "print(\"Resumen por categor√≠a (ordenado por ingreso_total):\")\n",
        "display(resumen_cat.head(20))\n"
      ],
      "metadata": {
        "id": "U84DDOyUDtJU"
      },
      "id": "U84DDOyUDtJU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üü¶ Punto 9 ‚Äî Integraci√≥n de datos (ventas + marketing)\n",
        "### Conceptos y plan\n",
        "\n",
        "Qu√© haremos\n",
        "\n",
        "Integraci√≥n (merge/join): unimos filas de dos tablas usando una clave de uni√≥n (misma entidad en ambos).\n",
        "\n",
        "Si los nombres de columnas no coinciden exactamente, los detectamos autom√°ticamente:\n",
        "\n",
        "Normalizamos nombres (min√∫sculas, sin tildes/espacios/s√≠mbolos) y buscamos intersecci√≥n.\n",
        "\n",
        "Si no hay intersecci√≥n, usamos b√∫squeda difusa (fuzzy) para proponer el mejor par.\n",
        "\n",
        "Si existen columnas de fecha en ambos, probamos clave compuesta (id_cliente + fecha).\n",
        "\n",
        "Si nada aplica, usamos un mapa manual KEY_MAP (ej.: {\"ventas\": \"cliente_id\", \"marketing\": \"id_cliente\"}).\n",
        "\n",
        "Hacemos merge con how='left' para no perder ventas aunque falte marketing.\n",
        "\n",
        "Luego, si hay columnas de campa√±a/canal, agregamos ingresos por cada una.\n",
        "\n",
        "Conceptos clave\n",
        "\n",
        "Join/merge: combinaci√≥n de tablas por clave(s) comunes.\n",
        "\n",
        "Tipos (how):\n",
        "\n",
        "left: conserva todas las filas de la izquierda; rellena marketing con NaN si no matchea.\n",
        "\n",
        "inner: conserva solo coincidencias.\n",
        "\n",
        "outer: uni√≥n completa (puede llenar NaN en ambos lados).\n",
        "\n",
        "right: conserva todo de la derecha.\n",
        "\n",
        "Cardinalidad (validate):\n",
        "\n",
        "1:1, 1:m, m:1, m:m (nosotros usamos m:m porque puede haber m√∫ltiples por clave).\n",
        "\n",
        "Conflictos de nombre (suffixes): si hay columnas con mismo nombre pero distinto significado, agregamos sufijos, p. ej. ('_ven','_mkt').\n",
        "\n",
        "¬øQu√© es integrar (merge/join) dos tablas?\n",
        "Es combinar filas de dos DataFrames usando una clave en com√∫n (columna que identifica la misma entidad en ambos). Es como un JOIN de SQL.\n",
        "\n",
        "Tipos de join (how en pd.merge)\n",
        "\n",
        "left: conserva todas las filas de la tabla izquierda; si no hay match en la derecha, rellena con NaN.\n",
        "\n",
        "inner: devuelve solo las coincidencias (intersecci√≥n).\n",
        "\n",
        "outer: devuelve todo (uni√≥n completa) y rellena con NaN donde falte.\n",
        "\n",
        "right: conserva todas las filas de la tabla derecha.\n",
        "\n",
        "¬øQu√© es la cardinalidad?\n",
        "Describe cu√°ntas filas puede haber por cada valor de clave en cada tabla:\n",
        "\n",
        "1:1 ‚Üí cada clave aparece como m√°ximo una vez en ventas y una vez en marketing.\n",
        "\n",
        "1:m ‚Üí ventas tiene la clave √∫nica (1) y marketing puede tener varias (m) filas por esa clave.\n",
        "\n",
        "m:1 ‚Üí ventas tiene varias por clave y marketing solo una.\n",
        "\n",
        "m:m ‚Üí ambos lados pueden tener varias filas por clave.\n",
        "¬øPor qu√© importa?\n",
        "\n",
        "Si esperabas 1:1 pero en realidad hab√≠a duplicados, el merge multiplica filas (efecto ‚Äúcartesiano‚Äù por clave repetida), alterando totales.\n",
        "\n",
        "pd.merge(..., validate='1:m') permite forzar la cardinalidad esperada y lanza error si no se cumple.\n",
        "\n",
        "Plan de la celda:\n",
        "\n",
        "Intentar detectar autom√°ticamente una clave com√∫n con nombres t√≠picos (id_cliente, cliente, email, id_campa√±a, sku, id_producto).\n",
        "\n",
        "Si no se encuentra, te dejo dos variables para que indiques la clave a mano (clave_ventas, clave_marketing).\n",
        "\n",
        "Detectar cardinalidad real (inspeccionando si hay duplicados en la/s clave/s) y configurar validate como 1:1, 1:m, m:1 o m:m.\n",
        "\n",
        "Hacer merge con how='left' (no perder ventas).\n",
        "\n",
        "Si existen columnas de campa√±a o canal, resumir ingresos por cada una."
      ],
      "metadata": {
        "id": "Ufe7wbTND5Tp"
      },
      "id": "Ufe7wbTND5Tp"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 9) INTEGRACI√ìN SIMPLE: combinar ventas y marketing\n",
        "# ============================================\n",
        "# Qu√© hace:\n",
        "# - Busca una clave com√∫n sencilla para unir (por nombre t√≠pico).\n",
        "# - Si no la encuentra, te deja dos variables para definirla a mano.\n",
        "# - Calcula la cardinalidad real (1:1, 1:m, m:1, m:m) y la valida en el merge.\n",
        "# - Hace un LEFT JOIN (conserva todas las ventas).\n",
        "# - Si hay 'campa√±a'/'canal', resume ingresos por esas columnas.\n",
        "# ============================================\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 1) Intento SIMPLE de detectar una clave com√∫n\n",
        "# ---------------------------\n",
        "claves_tentativas = [\n",
        "    \"id_cliente\", \"cliente\", \"email\",\n",
        "    \"id_campa√±a\", \"id_campana\", \"id_campanha\",\n",
        "    \"sku\", \"id_producto\", \"producto\"\n",
        "]\n",
        "\n",
        "# Busca la primera clave que exista con el mismo nombre en ambos DataFrames\n",
        "clave_comun = next(\n",
        "    (k for k in claves_tentativas if k in ventas_clean.columns and k in marketing_clean.columns),\n",
        "    None\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# 2) Si NO hay clave exactamente igual en ambos, pedimos definir a mano\n",
        "#    (esto NO corta el flujo: imprime gu√≠a y sigue si las completas)\n",
        "# ---------------------------\n",
        "# üëá Cambi√° estos nombres si tus columnas se llaman distinto en cada DF:\n",
        "clave_ventas = None   # ej.: \"id_cliente\" (en ventas_clean)\n",
        "clave_marketing = None  # ej.: \"cliente_id\" (en marketing_clean)\n",
        "\n",
        "# Si el usuario NO defini√≥ manualmente y TAMPOCO hay clave com√∫n, emitimos gu√≠a y salimos limpio\n",
        "if clave_comun is None and (clave_ventas is None or clave_marketing is None):\n",
        "    print(\"‚ùå No se encontr√≥ una clave com√∫n por nombre.\")\n",
        "    print(\"üëâ Opciones:\")\n",
        "    print(\"   a) Renombr√° una columna para que coincida en ambos DataFrames (ej.: 'id_cliente').\")\n",
        "    print(\"   b) Defin√≠ manualmente las variables 'clave_ventas' y 'clave_marketing' m√°s arriba.\")\n",
        "    print(\"      Ejemplo: clave_ventas='id_cliente'  |  clave_marketing='cliente_id'\")\n",
        "    # Evitamos romper el notebook\n",
        "else:\n",
        "    # ---------------------------\n",
        "    # 3) Determinar las columnas de uni√≥n y calcular cardinalidad REAL\n",
        "    # ---------------------------\n",
        "    if clave_comun is not None:\n",
        "        # Caso simple: misma columna en ambos\n",
        "        left_on = [clave_comun]\n",
        "        right_on = [clave_comun]\n",
        "        clave_label = clave_comun\n",
        "    else:\n",
        "        # Caso manual: columnas diferentes\n",
        "        left_on = [clave_ventas]\n",
        "        right_on = [clave_marketing]\n",
        "        clave_label = f\"{clave_ventas} (ventas) ‚Üî {clave_marketing} (marketing)\"\n",
        "\n",
        "    # Funci√≥n de ayuda: ¬øhay duplicados en la(s) clave(s)?\n",
        "    def hay_duplicados(df, cols):\n",
        "        # cols puede tener 1 o m√°s columnas (clave compuesta)\n",
        "        return df.duplicated(subset=cols, keep=False).any()\n",
        "\n",
        "    # Detectar cardinalidad:\n",
        "    # - Si ventas no duplica clave y marketing s√≠ ‚Üí 1:m\n",
        "    # - Si ventas s√≠ duplica y marketing no ‚Üí m:1\n",
        "    # - Si ninguno duplica ‚Üí 1:1\n",
        "    # - Si ambos duplican ‚Üí m:m\n",
        "    dup_left = hay_duplicados(ventas_clean, left_on)\n",
        "    dup_right = hay_duplicados(marketing_clean, right_on)\n",
        "\n",
        "    if not dup_left and not dup_right:\n",
        "        validate_card = \"1:1\"\n",
        "    elif not dup_left and dup_right:\n",
        "        validate_card = \"1:m\"\n",
        "    elif dup_left and not dup_right:\n",
        "        validate_card = \"m:1\"\n",
        "    else:\n",
        "        validate_card = \"m:m\"\n",
        "\n",
        "    print(\"üîë Clave de uni√≥n:\", clave_label)\n",
        "    print(\"üìê Cardinalidad detectada:\", validate_card)\n",
        "\n",
        "    # ---------------------------\n",
        "    # 4) Hacer el MERGE (LEFT JOIN)\n",
        "    # ---------------------------\n",
        "    ventas_marketing = pd.merge(\n",
        "        left=ventas_clean,      # DataFrame izquierdo: base principal (ventas)\n",
        "        right=marketing_clean,  # DataFrame derecho: datos de marketing a anexar\n",
        "        how=\"left\",             # how: 'left' (todas ventas), 'inner', 'outer', 'right'\n",
        "        left_on=left_on,        # columnas de uni√≥n en 'left' (lista)\n",
        "        right_on=right_on,      # columnas de uni√≥n en 'right' (lista)\n",
        "        # on=...               # (alternativa si la columna tiene EXACTAMENTE el mismo nombre en ambos; excluyente con left_on/right_on)\n",
        "        sort=False,             # sort: True ordena por clave tras el merge; False suele ser m√°s r√°pido\n",
        "        suffixes=(\"_ven\", \"_mkt\"),  # sufijos para columnas con el mismo nombre en ambos DF\n",
        "        copy=True,              # copy: True asegura copia (seguro); False puede ahorrar memoria\n",
        "        indicator=True,         # indicator: agrega columna '_merge' con 'left_only'|'right_only'|'both'\n",
        "        validate=validate_card  # validate: '1:1'|'1:m'|'m:1'|'m:m' ‚Üí lanza error si no se cumple\n",
        "    )\n",
        "\n",
        "    # Diagn√≥stico b√°sico del resultado\n",
        "    print(\"\\nüìã Origen de filas seg√∫n '_merge':\")\n",
        "    display(ventas_marketing['_merge'].value_counts(dropna=False).to_frame('conteo'))\n",
        "\n",
        "    print(\"\\nüëÄ Primeras filas del DataFrame unificado:\")\n",
        "    display(ventas_marketing.head())\n",
        "\n",
        "    # ---------------------------\n",
        "    # 5) Resumen por CAMPA√ëA / CANAL (si existen esas columnas)\n",
        "    # ---------------------------\n",
        "    # Asegurar 'ingreso' = precio * cantidad si no existe\n",
        "    vm = ventas_marketing.copy()\n",
        "    if \"ingreso\" not in vm.columns and {\"precio\", \"cantidad\"}.issubset(vm.columns):\n",
        "        vm = vm.assign(ingreso = vm[\"precio\"] * vm[\"cantidad\"])\n",
        "\n",
        "    # Detectar posibles columnas de campa√±a/canal por nombres comunes\n",
        "    def hallar_col(df, patrones):\n",
        "        for c in df.columns:\n",
        "            if any(p in c.lower() for p in patrones):\n",
        "                return c\n",
        "        return None\n",
        "\n",
        "    camp_col  = hallar_col(vm, [\"campa√±a\", \"campana\", \"id_campa√±a\", \"id_campana\", \"id_campanha\", \"campaign\"])\n",
        "    canal_col = hallar_col(vm, [\"canal\", \"utm_source\", \"fuente\", \"source\"])\n",
        "\n",
        "    if camp_col and \"ingreso\" in vm.columns:\n",
        "        resumen_camp = (\n",
        "            vm.groupby(by=camp_col, dropna=False, as_index=False)  # groupby: agrupa por campa√±a\n",
        "              .agg(\n",
        "                  ingreso_total=('ingreso', 'sum'),  # sum: suma de ingresos por campa√±a\n",
        "                  ventas=('ingreso', 'size')         # size: cantidad de filas (ventas) por campa√±a\n",
        "              )\n",
        "              .sort_values(by='ingreso_total', ascending=False, na_position='last', ignore_index=True)\n",
        "        )\n",
        "        print(f\"\\nüí° Ingreso total por campa√±a ({camp_col}):\")\n",
        "        display(resumen_camp.head(20))\n",
        "    else:\n",
        "        print(\"\\n‚ÑπÔ∏è No se encontr√≥ columna de campa√±a o no est√° 'ingreso' para resumir.\")\n",
        "\n",
        "    if canal_col and \"ingreso\" in vm.columns:\n",
        "        resumen_canal = (\n",
        "            vm.groupby(by=canal_col, dropna=False, as_index=False)\n",
        "              .agg(\n",
        "                  ingreso_total=('ingreso', 'sum'),\n",
        "                  ventas=('ingreso', 'size')\n",
        "              )\n",
        "              .sort_values(by='ingreso_total', ascending=False, na_position='last', ignore_index=True)\n",
        "        )\n",
        "        print(f\"\\nüí° Ingreso total por canal ({canal_col}):\")\n",
        "        display(resumen_canal.head(20))\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è No se encontr√≥ columna de canal o no est√° 'ingreso' para resumir.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "gTZbdP_MD9xo"
      },
      "id": "gTZbdP_MD9xo",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}